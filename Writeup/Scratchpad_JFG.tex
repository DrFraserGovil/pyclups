\documentclass[]{article}
\usepackage{JML}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage[left=1in,right=1in]{geometry}
\title{Jack's Scratchpad}
\setlength\parskip{5pt}
\setlength\parindent{0pt}
\def\llangle{\left\langle}
\def\rrangle{\right\rangle}
\newcommand\E[1]{\llangle #1 \rrangle}
\usepackage{tikzsymbols}
\newcommand\T[1][i]{\mathcal{T}_{#1}}
\def\a{\vec{a}_t}
\def\ai{\vec{a}_{t_i}}
\def\vi{\vec{v}_i}
\def\wi{\vec{w}}
		
\begin{document}
	\maketitle
	\tableofcontents
	\section{Initialisation}

		In the case where $\vec{w}$ must be optimised, we must first choose an initial starting point, $\vec{w}_0$, for the starting optimisation. The closer this is to the optimal point, the quicker the optimiser will converge.

		In order to make this efficient, we rewrite the constraint vector $\vec{c}$ in the following fashion:
		\begin{equation}
			\vec{c}(\vec{w}) = \vec{\xi} + \psi(\vec{w})
		\end{equation}
		Here $\vec{\xi}$ contains both the equality constraints and any constant-offsets associated with the inequality constraints, such that we may then enforce the following conditions on $\vec{\psi}$:
		\begin{equation}
			\left[ \vec{\psi}(\vec{w}) \right]_i  \begin{cases}
					= 0 & \text{if $i$ exact constraint}
					\\
					\geq 0 & \text{else}
				\end{cases}
		\end{equation}
		For example, if condition $j$ is that $x_j \geq -4$, then $\xi_j = -4$ and $\psi_j = \exp(w_j)$. We also limit ourselves to the case where $\vec{w} = \psi^{-1}(\vec{c} - \vec{\xi})$ exists. Note that this is not a limitation on our general method, but rather a choice made for efficient initialisation.

		The algorithm for determining the initialisation point is then:
		\begin{enumerate}
			\item Compute $\{ \vec{a}^\text{BLUP} \}$ and hence $\hat{\vec{Z}}^\text{blup}$: the predictors using the normal BLUP algorithm
			\item Let $\tilde{\vec{c}} = B \vec{p}_t = \vec{\xi} + \tilde{\vec{\varphi}}$
			\item Project onto the constraint-meeting surface:
			$$ \varphi_j = \begin{cases} \tilde{\varphi}_j & \text{if } \tilde{\varphi}_j \geq 0
				\\
				0 & \text{else} \end{cases}$$
			\item Set $\vec{w}_0 = \psi^{-1}\left(\varphi_j\right)$
		\end{enumerate}
		In practice, a small amount of numerical tolerance might be required (setting a $\varphi_j =0$ when $\psi^{-1} = \ln(\varphi)$ is not numerically stable), so at step 3 we suggest setting $\varphi_j = \epsilon$, some very small numerical quantity.

		\subsection{Comments on Initialisation}

			This method of initialisation is a na\"ive projection from the BLUP onto the space of constraint-obeying functions. In some simple cases, this projection is in fact equal to the global maximum: the case of positive functions, for example - the na\"ive projection truncates the BLUP to be equal to 0 wherever the BLUP would become negative, which is exactly the global solution. 
			
			In some pathological cases, however, this projection might lead to a function extremely far away from both the global maximum and the BLUP: consider the case of a BSCLUP constrained to be monotonically increasing, but where the BLUP is monotonically \textit{decreasing}. In this case, the projection of $\vec{w}_0$ would result in a flat line at the height of $Z^{BLUP}_0$ -- a rather significant deviation, and unlikely to be close to the optimum.

			Experimentally, we find that this initialisation serves as a good initial \textit{ansatz} as to the location of the optimum point for most real-world applications.
			

	\section{Thoughts on Paper V2}
		I like the newest version of the paper a lot! Much easier to read in some of the key areas. Here are some thoughts:

	
		\begin{enumerate}
			\item I still don't like the name BSCLUP -- how about we meet halfway and go with CLUPS: \textit{Constrained Linear Unbiased Predictor-Sequences/Sets}? 
			\item Footnote 1 has a question in it. (`random variables $X$ and $Y$ is CLUP thoughts'). Not sure I understand the question -- seems fine to end after $Y$?
			\item The note about the BSCLP vs BSCLUP is very good -- I wonder if a similar note about the BLP is worthwhile at the start of Section 2 -- emphasising \textit{why} we care about this entity, something along the lines of:
			
			``The \textit{best linear predictor} (BLP) for $Z_t$ is the linear predictor $\hat{Z}_t^\text{BLUP}$ which minimises the MSE subject to no constraints, and therefore has the minimum MSE among all linear predictors. Mechanically, the derivation is similar to above, but with the important conceptual difference of using the second moment $R = ...$ instead of the covariance (right?)."
			\item I think rewording the opening sentence of section 3 might place the emphasis more on what we are trying to do (and why it matters), something like:
			
			``In the BLP and BLUP, each prediction is treated as an individual point, distinct from all others. In practice, however, we are often interested in constructing sets and sequences of predictions upon which we might wish to impose constraints. In the case of a set of predictors, we might wish to impose upper or lower bounds, whilst in the case of a sequence we might wish to impose constraints between individual predictions such as monotonicity. Such sequence-constraints are non-separable since we cannot optimise one prediction without potentially impacting another in the sequence, and so we are moved to consider predictor-sequences as entities in their own right."
			\item Worth noting in the BSCLUP /BSCLP note (which I like!) that the distinction is conceptual (moments vs covariances) as well?
			\item Just before eq. 32, I think it is incorrect to say that we limit ourselves to the case of linear constraints to make the derivatives analytically (trivially not the case: quadratic constraints still have analytical derivatives!) it is that the derivatives can be \textit{analytically solved to produce Eq. 52} 
			\item I have come up with a better way to say the bit about the KKT constraints:
			
			``Since there is no guarantee of convexity, standard approaches such as slack variables and the Karush-Kuhn-Tucker conditions are not always applicable. As a general solution we parameterize the constraints such that"

			\item The text above Eq. 38 is weirdly archaic, try \textit{The random vector $X$ can be decomposed as in Eq. 8...}
			\item I wrote down `should we maybe capitalise (In)Exact' -- make it an Important Property that draws the eye?
			\item After the description of item 1 of the itemised list of ways the gradient can be 0, it might be useful to direct readers straight to remark 6:
			``The first case holds.... (see Eq 40 \textbf{and remark 6})''?
			\item In 3.1 (just before Eq. 57), you bother to remind me that $B$ is of size $q\times m$, but not what $q$ and $m$ are -- worthwhile doing so, to prevent me flicking backwards and forwards
			\item I would say that the ending of the first part of 3.1 could be made more obvious if you explicitly listed the ways in which $BB^T$ can be rank deficient (as you do with the zero-gradient conditions):
			
			``If any two rows are linearly dependent (for example, if any two of $B_1, B_2$ and $B_3$ are equal) then $B$ is rank deficient and $BB^T$ is uninvertible. There are four possible causes of this behaviour
			\begin{enumerate}
				\item Contradictory constraints
				\item Redundant Constraints
				\item Excess constraints
				\item Homonym Constraints (\textbf{name?})
			\end{enumerate}
			These first two causes indicate that the statistician has formulated their problem poorly, whilst the final two can arise even from well-formulated statements, and may require some cunning of the part of the statistician to rewrite their conditions in an appropriate fashion.''

			\item I would maybe put the sums in `Contradictory Constraints' next to their text -- it reads a bit weirdly at the moment:
			
			``For example, we may not require that the sum of the elements of the BSCLUP be both zero ($\sum_i \hat{Z} = 0$) and one ($\sum_i \hat{Z} = 1$) since these are inconsistent.''

			Follow that same pattern through the rest? Maybe just my personal preference

			\item In `Excess Constraints' I would emphasise that this is not necessarily a `you gone messed up' problem -- it's obvious when expressed in this language that it fails, but not necessarily obvious that $>0$ and $\sum = 1$ should fail on these grounds. 
			
			\item `Other Constraints' -- don't like the name (I suggested homonyms above, and I will write a suggestion based on that name)
			
			``It is possible for $BB^T$ to be singular even when the constraints are well formed and neither contradictory, redundant nor in excess. This arises when the constraints are expressed as `homonyms' of a constraint which \textit{would} be contradictory or redundant - that is, a set of constraints $B_i = \sum_{j\neq i} \alpha_j B_j$ but $\vec{c}_i \neq \sum_{j\neq i} \alpha_j \vec{c}_j$, such that the constraint `sounds' the same, but has a different meaning.
			
			For example, we consider a BSCLUP of $m > 2$ elements, where the only constraint is that $0 \leq \hat{Z}_0 \leq 1$. In this case, it would be natural to write the $2\times m$ matrix $B$ as:
			\begin{align}
				\begin{split}
					B &= \begin{pmatrix}
						1 & 0 & \hdots & 0
						\\
						1 & 0 & \hdots & 0
					\end{pmatrix}
				\end{split}
			\end{align}
			And the associated $\vec{c}$ as
			\begin{align}
				\begin{split}
					\vec{c}(\vec{w}) = \begin{pmatrix}
						\exp(w_1)
						\\
						1 - \exp(w_2)
					\end{pmatrix}
				\end{split}
			\end{align}
		
		This is a well formulated, valid constraint with fewer constraints than predictions - and yet it is clear that $BB^T$ is uninvertible. We can understand why this is by considering the invalid constraint $0 \leq Z_0 \leq -1$, the resulting $B^\prime$ and $\vec{c}^\prime$ are:
		\begin{align}
			% \begin{split}
				B^\prime &= \begin{pmatrix}
					1 & 0 & \hdots & 0
					\\
					1 & 0 & \hdots & 0
				\end{pmatrix}
				\\
				\vec{c}^\prime(\vec{w})& = \begin{pmatrix}
					\exp(w_1)
					\\
					-1 - \exp(w_2)
				\end{pmatrix}
			% \end{split}
		\end{align}
		We can see that the only difference between our valid and invalid constraints were encapsulated within $\vec{c}$. From the perspective of $BB^T$, both valid and invalid constraints are identical - thus we say that the valid constraint is a \textit{homonym} of the invalid constraint. 

		In order to generate a valid $B$, it is necessary only to formulate $B$ and $\vec{c}$ in a way which breaks this relationship - one potential solution would be:
		\begin{align}
			B & = \begin{pmatrix}
				1 & 0 & \hdots & 0
			\end{pmatrix}
			\\
			\vec{c}(w) & = \begin{pmatrix}
				\frac{1}{1 + \exp(-w)}
			\end{pmatrix}
		\end{align}

		\item There is a block of text in the second paragraph of 4 -- the BSCLUP being of finite length = resolution dependent. I think this is way too important a facet of the theory to be put here (it has implications for the error bounds, for example) - I would suggest maybe giving it its own dedicated (sub)section earlier in the paper.
		\item In Eq. 83 I think you actually want $t_i$ instead of $i$ -- you've implicitly assumed a) symmetry and b) ordering of the prediction points!
		\item In these examples, is it worth explicitly writing how these $B$/$c$ combinations result in the constraint being obeyed? I think it's a bit opaque as to how 85 and 86 result in monotonicity, but if we added in a line which said:
		\begin{equation}
			\hat{Z}_i = \hat{Z}_{i-1} + e^{z_i}
		\end{equation}
		Then it might be obvious how to go from a to b.

		\end{enumerate}

	\section{Prediction Errors}
		
		\subsection{Why the BLUP Approach doesn't work}
			
			The approach in standard BLUP texts is to simply use that the prediction error is (approximately -- some assumptions needed if I recall?) equal to the MSE evaluated at the optimum.

			This, however, utilises the assumption that each of the prediction points is independent; an assumption that does not follow through with the BSCLUP. We have emphasised that the BSCLUP prediction is on the \textit{entire} sequence/series of points - and hence any associated error must be computed on a global scale. 

			Simply put, it does not make sense to think about the error associated with just one point, when moving that point might have an impact on subsequent points (i.e., it is \textit{impossible} to move a point upwards in a monotonic predictor-series if the subsequent point already has the same prediction value, as this would violate the constraint.) 
			
			This concern is not merely limited to the predictor-sequences, as predictor-series also violate the assumptions that allow the MSE to be used; a trivial example would be the error on a predictor-series constrained to be non-negative, but which is predicted to be equal to zero. It is evident that a symmetric error around $Z = 0$ would not be representative of the predictor error at that point.
			
			We must therefore lend slightly more care and attention to our errors.
			
		\subsection{The MCMC Approach}

			Errors on sequences naturally lend themselves to an MCMC-style approach, as this provides a natural way to explore the intercorrelation between the sequence/series.

			In an ideal scenario, we would simply vary the predictor values, $\{\hat{Z}_\text{clup}\}$, and use this to generate a score $\mathcal{L}$ which the MCMC engine could explore. This faces two major problems:
			\begin{enumerate}
				\item The score function $\mathcal{L}$ is expressed in terms of $\{{\vec{a}}\}$, but $\hat{Z}$ and $\vec{a}$ are related through a non-invertible dot-product.
				\item With complex constraints, the majority of proposed variations to $\hat{Z}$ would be invalid, and hence the MCMC engine would not be able to produce a reliable chain.
			\end{enumerate}

			We must therefore run the MCMC engine in $\vec{a}$-space; which has the unfortunate by-product of being much higher-dimensional, and therefore has a higher autocorrelation length. However, blindly proposing a new $\vec{a}$ falls afoul of point 2) raised above, namely that the majority of the time, the resulting predictions will not be valid. 


			I therefore propose 4 potential algorithms for generating a valid MCMC chain.

			\subsubsection*{Algorithm 1: ``Fuck You, Markov, You Don't Know Me''}
				
				This algorithm is simple: any proposed $\{\vec{a}\}$ which violate the constraints is given a score of $-\infty$, and the rest is left up to the MCMC engine to handle. 

				This \textit{might} work in some of the inequality cases -- it almost certainly won't work in exact constraints (i.e. the probability of the MCMC generating a curve with an integral equal to 1 (within machine precision) is vanishingly small).

				I do not recommend this, but it is technically an option.

			\subsubsection*{Algorithm 2: ``Exactitude''}

				In this case, we treat the variation as happening on the space of $\{a_\text{nqblup} \}$ (nqBLUP = not-quite-best LUP, since we have varied it away from the optimum!). If the constraints were exact, then this is almost identical to simply varying the $\{\vec{a}\}$, you simply have to correct the predictor using the BSCLUP identity. If the constraints are inexact, then for each proposed $\{ a_\text{nqblup}\}$ we compute the exact value of $\vec{c}$ which optimises the predictor; we then have a means of associating a variational score to a predictor which is away from the mean, but which is guaranteed to obey the correct behaviour. 

				This is probably the most theoretically justifiable algorithm; the variables within $\vec{w}$ were always a fiction and so 'optimising them away' to produce the 'optimised-variation' seems like the best approach. 

				The downside is that -- aside from exact constraints and certain trivial cases -- this is computationally very costly, and will take a vast amount of computing power to produce meaningful results.
				
			\subsubsection*{Algorithm 3: ``Dual Variation''}

				It is clear that the MCMC must vary $\vec{a}_\text{nqblup}$ in order to produce meaningful results - however, we might take objection to the optimisation of $\vec{w}$ which the ``Exactitude'' method - firstly on practical grounds, and secondly on the idea that we are explicitly varying \textit{away} from the optimum -- so why do we not also vary $\vec{w}$\footnote{I don't know if I believe this, but would be interested in some thoughts!}?

				In this case, we form a composite vector $\{\vec{a}_\text{nqblup}, \vec{w}\}$ such that for each variation we can construct a $\vec{c}$, and then through the BSCLUP identity a $\vec{a}_\text{nqBSCLUP}$ and hence a score.

				The downside of this is that:
				\begin{itemize}
					\item We might argue the opposite way and say that unoptimised $\vec{w}$ values are meaningless
					\item This increases the number of dimensions (potentially up to twice as many), and so increases the autocorrelation time.
				\end{itemize}

			\subsubsection*{Algorithm 4: `Eh, Close Enough'}

				This final algorithm works similarly to Algorithm 2, except that no direct optimisation is involved. After proposing a new $\{\vec{a_\text{nqblup}}\}$, you then perform the Initialisation Projection:

				\begin{enumerate}
					\item Compute $\vec{\hat{\vec{Z}}}^\text{nqblup}$ using the normal BLUP algorithm
					\item Let $\tilde{\vec{c}} = B \vec{\hat{\vec{Z}}}^\text{nqblup} = \vec{\xi} + \tilde{{\varphi}}(\vec{w})$
					\item Project onto the constraint-meeting surface:
					$$ \varphi_j = \begin{cases} \tilde{\varphi}_j & \text{if } \tilde{\varphi}_j \geq 0
						\\
						0 & \text{else} \end{cases}$$
					\item Then set $\vec{c}^\prime = \vec{\xi} + \vec{\varphi}$
					\item Use $\{\vec{a}_\text{nqblup}\}$ and $\vec{c}^\prime$ to construct a $\{\vec{a}_\text{nqbsclup}\}$
				\end{enumerate}

				This guarantees that all proposed $\{\vec{Z}\}$ obey the constraints, however the projection performed is somewhat naive and may sometimes be far away from the optimum. 

				However, since we are varying $\vec{a}_\text{nqblup}$ freely, it can move very far away from the optimum, and so it is possible to generate arbitrary constraint-obeying $\vec{a_\text{bsclup}}$ (i.e., although the projection of $\vec{a}_\text{blup}$ is not guaranteed to be near the optimum, if we set $\vec{a}_\text{nqblup} = \vec{a}_\text{bsclup}$, the projection would trivially be equal to $\vec{a}_\text{bsclup}$, and therefore small variations from this position will also be projected into small variations from the optimum.)

				This has the benefit of being able to explore arbitrary predictors (given enough time), without producing too many additional dimensions -- the downside is that since the projections may make many $\vec{a}_\text{nqblup}$ produce the same $\vec{a}_\text{bsclup}$ (and hence the same score), the MCMC might think it has redundant dimensions, get confused, or otherwise have an excessively high autocorrelation time as it struggles to find which parameters are meaningful.
\end{document}